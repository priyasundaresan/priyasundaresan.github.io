<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Priya Sundaresan</title>
  
  <meta name="author" content="Priya Sundaresan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Priya Sundaresan</name>
              </p>
              <p>I am a 4th year PhD student at Stanford University, co-advised by Professors <a href="https://dorsa.fyi/">Dorsa Sadigh</a>  and <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>. I am interested in imitation learning and representation learning for real-world manipulation. I am funded by the <a href="https://www.nsfgrfp.org/">National Science Foundation Graduate Fellowship</a>.
              </p>
              <p>Before Stanford, I completed my B.S. and M.S. at UC Berkeley, where I worked with Professor <a href='goldberg.berkeley.edu'>Ken Goldberg</a> in the <a href='autolab.berkeley.edu'>AUTOLAB</a>. I have also previously spent summers interning at Amazon Robotics and [Google] Intrinsic.</p>
              <p>Please feel free to reach out!</p>
              <p style="text-align:center">
                <a href="mailto:priyasun@stanford.edu">priyasun@stanford.edu</a> &nbsp/&nbsp
                <a href="data/PriyaSundaresanCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/priyasundaresan/">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=7SUquR4AAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/priyasun_?lang=en">X</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/PriyaSundaresan.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/PriyaSundaresan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
	     <video autoplay loop muted width="280">
	       <source src="images/sphinx.mp4" type="video/mp4">
	       Your browser does not support the video tag.
	     </video>
            </td>
            <td width="75%" valign="middle">
              <a href="https://sphinx-manip.github.io/">
                <papertitle>What's the Move? Hybrid Imitation Learning via Salient Points</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan*</strong>, Hengyuan Hu*, Quan Vuong, Jeannette Bohg, Dorsa Sadigh
              
              <br>
              <em>To Appear, International Conference on Learning Representations (ICLR), 2025.</em>
              <br>
              <a href="https://sphinx-manip.github.io/">Website</a> / <a href="https://arxiv.org/abs/2412.05426">PDF</a> 
            </td>
         </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
              <img src="images/mtpi.gif" alt="mtpi" width="280">
            </td>
            <td width="75%" valign="middle">
              <a href="https://portal-cornell.github.io/motion_track_policy/">
                <papertitle>Motion Tracks: A Unified Representation for Human-Robot Transfer in Few-Shot Imitation Learning</papertitle>
              </a>
              <br>
              Juntao Ren, <strong>Priya Sundaresan</strong>, Dorsa Sadigh, Sanjiban Choudhury, Jeannette Bohg
              
              <br>
              <em>Under Review, 2024.</em>
              <br>
              <a href="https://portal-cornell.github.io/motion_track_policy/">Website</a> / <a href="https://portal-cornell.github.io/motion_track_policy/">PDF</a> 
            </td>
         </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/flair.gif" alt="flair" width="280" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://flair-robot.github.io/assets/flair.pdf">
                <papertitle>FLAIR: Feeding via Long-horizon AcquIsition of Realistic dishes</papertitle>
              </a>
              <br>
              Rajat Kumar Jenamani*, <strong>Priya Sundaresan*</strong>, Maram Sakr, Tapomayukh Bhattacharjee‚Ä†, Dorsa Sadigh‚Ä†
              
              <br>
              <em>Robotic Science and Systems (RSS), 2024.</em>
              <br>
              <a href="https://flair-robot.github.io/">Website</a> / <a href="https://flair-robot.github.io/assets/flair.pdf">PDF</a>  / <a href="https://www.sfchronicle.com/tech/article/stanford-robotics-lab-home-helpers-19869354.php">SF Chronicle</a>
            </td>
         </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rtsketch.gif" alt="rtsketch" width="280" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://rt-sketch.github.io">
                <papertitle>RT-Sketch: Goal-Conditioned Imitation Learning from Hand-Drawn Sketches</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan</strong>, Quan Vuong, Jiayuan Gu, Peng Xu, Ted Xiao, Sean Kirmani, Tianhe Yu, Michael Stark, Ajinkya Jain, Karol Hausman, Dorsa Sadigh*, Jeannette Bohg*, Stefan Schaal* 
              <br>
              <em>Conference on Robot Learning (CoRL), 2024.</em> <br><font color="#30a650"><strong>‚òÖOral Presentation‚òÖ</strong></font>
              <br>
              <a href="https://rt-sketch.github.io">Website</a> / <a href="https://rt-sketch.github.io/assets/rt_sketch.pdf">PDF</a> / <a href="https://venturebeat.com/automation/deepmind-and-stanfords-new-robot-control-model-follow-instructions-from-sketches/">VentureBeat</a> 
            </td>
         </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rttraj.gif" alt="rttraj" width="280" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2311.01977">
                <papertitle>RT-Trajectory: Robotic Task Generalization via Hindsight Trajectory Sketches</papertitle>
              </a>
              <br>
              Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, <strong>Priya Sundaresan</strong>, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao
              <br>
              <em>International Conference on Learning Representations (ICLR), 2024. <font color="#30a650"></em><strong>‚òÖSpotlight (5%)‚òÖ</strong></font>
              <br>
              <a href="https://rt-trajectory.github.io/">Website</a> / <a href="https://rt-trajectory.github.io/pdf/RT_Trajectory.pdf">PDF</a> 
            </td>
         </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rtx.gif" alt="rttraj" width="280" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://robotics-transformer-x.github.io/">
                <papertitle>Open X-Embodiment: Robotic Learning Datasets and RT-X Models</papertitle>
              </a>
              <br>
              Open X-Embodiment Collaboration [>150 authors]
              <br>
              <em>International Conference on Robotic Automation (ICRA), 2024.</em><br><font color="#30a650"></em><strong>‚òÖBest Paper Award‚òÖ</strong></font>
              <br>
              <a href="https://robotics-transformer-x.github.io/">Website</a> / <a href="https://arxiv.org/abs/2310.08864">PDF</a> 
            </td>
         </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/kite.gif" alt="kite" width="280" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2306.16605">
                <papertitle>KITE: Keypoint-Conditioned Policies for Semantic Manipulation</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan</strong>, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg
              <br>
              <em>Conference on Robot Learning (CoRL), 2023.</em>
              <br>
              <a href="http://tinyurl.com/kite-site">Website</a> / <a href="https://arxiv.org/pdf/2306.16605.pdf">PDF</a> 
            </td>
          </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spaghetti.gif" alt="spaghetti" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2309.05197">
                <papertitle>Learning Sequential Acquisition Policies for Robot-Assisted Feeding</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan</strong>, Jiajun Wu, Dorsa Sadigh
              <br>
              <em>Conference on Robot Learning (CoRL), 2023.</em>
              <br>
              <a href="https://sites.google.com/view/vaporsbot">Website</a> / <a href="https://arxiv.org/pdf/2309.05197.pdf">PDF</a> 
            </td>
          </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bite_transfer.gif" alt="bite transfer" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2211.12705">
                <papertitle>In-Mouth Robotic Bite Transfer with Visual and Haptic Sensing</papertitle>
              </a>
              <br>
              Lorenzo Shaikewitz*, Yilin Wu*, Suneel Belkhale*, Jennifer Grannen, <strong>Priya Sundaresan</strong>, Dorsa Sadigh
              <br>
              <em>International Conference on Robotics and Automation (ICRA), 2023.</em> 
              <br>
              <a href="https://sites.google.com/view/bitetransfericra2023/home">Website</a> / <a href="https://arxiv.org/pdf/2211.12705.pdf">PDF</a> 
            </td>
          </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/skewering.gif" alt="skewering" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2211.14648">
                <papertitle>Learning Visuo-Haptic Skewering Strategies for Robot-Assisted Feeding</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan</strong>, Suneel Belkhale, Dorsa Sadigh
              <br>
              <em>Conference on Robot Learning (CoRL), 2022.</em> <br><font color="#30a650"><strong>‚òÖOral Presentation‚òÖ</strong></font>
              <br>
              <a href="http://tinyurl.com/robotskewering">Website</a> / <a href="https://arxiv.org/pdf/2211.14648.pdf">PDF</a> / <a href="https://hai.stanford.edu/news/building-precise-assistive-feeding-robot-can-handle-any-meal">Stanford HAI Blogpost</a>
            </td>
          </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/diffcloud.gif" alt="diffcloud" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2204.03139">
                <papertitle>DiffCloud: Real-to-Sim from Point Clouds with Differentiable Simulation and Rendering of Deformable Objects</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan</strong>, Rika Antonova, Jeannette Bohg
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2022.</em>
              <br>
              <a href="https://sites.google.com/berkeley.edu/diffcloud/home">Website</a> / <a href="https://arxiv.org/abs/2204.03139">PDF</a> 
            </td>
          </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ral2022.gif" alt="bayes-real2sim" width="280" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2112.05068">
                <papertitle>A Bayesian Treatment of Real-to-Sim for Deformable Object Manipulation</papertitle>
              </a>
              <br>
              Rika Antonova, Jingyun Yang, <strong>Priya Sundaresan</strong>, Dieter Fox, Fabio Ramos, Jeannette Bohg
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L), 2022.</em>
              <br>
              <a href="https://arxiv.org/abs/2112.05068">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros2021.gif" alt="mult-cable-untangling" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2106.02252">
                <papertitle>Disentangling Dense Multi-Cable Knots</papertitle>
              </a>
              <br>
               Vainavi Viswanath*, Jennifer Grannen*, <strong>Priya Sundaresan*</strong>, Brijen Thananjeyan, Ashwin Balakrishna, Ellen Novoseller, Jeffrey Ichnowski, Michael Laskey, Joseph E. Gonzalez, Ken Goldberg
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), 2021.</em>
              <br>
              <a href="https://sites.google.com/view/multi-cable-disentangling">Website</a> / <a href="https://arxiv.org/pdf/2106.02252.pdf">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/rss2021.gif" alt="nonplanar-untangling" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://roboticsconference.org/program/papers/013/">
                <papertitle>Untangling Dense Non-Planar Knots by Learning Manipulation Features and Recovery Policies</papertitle>
              </a>
              <br>
               <strong>Priya Sundaresan*</strong>, Jennifer Grannen*, Brijen Thananjeyan, Ashwin Balakrishna, Jeffrey Ichnowski, Ellen Novoseller, Minho Hwang, Michael Laskey, Joseph E. Gonzalez, Ken Goldberg
              <br>
              <em>Robotics: Science and Systems (RSS), 2021.</em>
              <br>
              <a href="https://tinyurl.com/rssuntangling">Website</a> / <a href="http://www.roboticsproceedings.org/rss17/p013.pdf">PDF</a>
            </td>
          </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icra2021.gif" alt="descriptors-fabric" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2003.12698">
                <papertitle>Learning Dense Visual Correspondences in Simulation to Smooth and Fold Real Fabrics</papertitle>
              </a>
              <br>
              Aditya Ganapathi, <strong>Priya Sundaresan</strong>, Brijen Thananjeyan, Ashwin Balakrishna, Daniel Seita, Jennifer Grannen, Minho Hwang, Ryan Hoque, Joseph E. Gonzalez, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg
              <br>
              <em>International Conference on Robotics and Automation (ICRA), 2021.</em>
              <br>
              <a href="https://sites.google.com/view/fabric-descriptors">Website</a> / <a href="https://arxiv.org/pdf/2003.12698.pdf">PDF</a>
            </td>
          </tr>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iros2020-wkshop.gif" alt="mmgsd" width="280" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2010.04339">
                <papertitle>MMGSD: Multi-Modal Gaussian Shape Descriptors for Correspondence Matching in 1D and 2D Deformable Objects.</papertitle>
              </a>
              <br>
              <strong>Priya Sundaresan*</strong>, Aditya Ganapathi*, Brijen Thananjeyan, Ashwin Balakrishna, Daniel Seita, Ryan Hoque, Joseph Gonzalez, Ken Goldberg
              <br>
              <em>International Conference on Intelligent Robots and Systems (IROS), Workshop on Robotic Manipulation of Deformable Objects, 2020.</em>
              <br>
              <a href="https://arxiv.org/abs/2010.04339">PDF</a>
            </td>
          </tr>


         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/corl2020_gif.gif" alt="untangling" width="280" height="l80">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2011.04999">
                <papertitle>Untangling Dense Knots by Learning Task-Relevant Keypoints</papertitle>
              </a>
              <br>
              Jennifer Grannen*, <strong>Priya Sundaresan*</strong>, Brijen Thananjeyan, Jeffrey Ichnowski, Ashwin Balakrishna, Minho Hwang, Vainavi Viswanath, Michael Laskey, Joseph E. Gonzalez, Ken Goldberg
              <br>
              <em>Conference on Robot Learning (CoRL), 2020.</em> <br><font color="#30a650"><strong>‚òÖOral Presentation‚òÖ</strong></font>
              <br>
              <a href="https://sites.google.com/berkeley.edu/corl2020ropeuntangling/home">Website</a> / <a href="https://arxiv.org/pdf/2011.04999.pdf">PDF</a>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/icra2020.gif" alt="rope-manip" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2003.01835">
                <papertitle>Learning Rope Manipulation Policies Using Dense Object Descriptors Trained on Synthetic Depth Data.</papertitle>
              </a>
              <br>
               <strong>Priya Sundaresan</strong>, Jennifer Grannen, Brijen Thananjeyan, Ashwin Balakrishna, Michael Laskey, Kevin Stone, Joseph E. Gonzalez, Ken Goldberg.
              <br>
             <em>International Conference on Robotics and Automation (ICRA), 2020.</em>
              <br>
              <a href="https://sites.google.com/view/ropemanipdescriptors/home">Website</a> / <a href="https://arxiv.org/pdf/2003.01835.pdf">PDF</a>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/case2019.gif" alt="needle-extraction" width="280" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://goldberg.berkeley.edu/pubs/2019-CASE-sundaresan-needle-extraction-to-appear.pdf">
                <papertitle>Automated Extraction of Surgical Needles from Tissue Phantoms</papertitle>
              </a>
              <br>
               <strong>Priya Sundaresan</strong>, Brijen Thananjeyan, Johnathan Chiu, Danyal Fer, Ken Goldberg
              <br>
              <em>Conference on Automation Science and Engineering (CASE), 2019.</em>
              <br>
              <a href="https://goldberg.berkeley.edu/pubs/2019-CASE-sundaresan-needle-extraction-to-appear.pdf">PDF</a>
            </td>
          </tr>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>For Fun</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/doge_dodge.png" alt="diffcloud" width="280" height="30">
            </td>
            <td width="75%" valign="middle">
              <a href="game.html">
                <papertitle>Doge Dodge</papertitle>
              </a>
              <br>This is a silly little game I made which is heavily inspired by the Chrome T-Rex game that pops up when there's bad connectivity (s/o to UC Berkeley's wifi!), as well as my favorite game of all time, Crossy Road!! 
              <br>
              Credits: <a href="https://ebiten.org/">tile art and sound effects</a>, <a href="http://pixelartmaker.com/art/13d23ac43116c0c.png">doge art</a>, <a href="http://pixelartmaker.com/art/dc64eddc1719fb3.png">cactus art</a>, <a href="https://steamuserimages-a.akamaihd.net/ugc/96104387823325173/12EBB3BF7F176EFFCE809BFA500953E1F60F8825/">background art</a>, <a href="http://pixelartmaker.com/art/da268f06e621b21.png">rock art</a>, <a href="http://pixelartmaker.com/art/dca2574a41f6294.png">spike art</a>.
              <br>
            </td>
          </tr>
        </tbody></table>

        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Shoutout to Jon Barron for the <a href="https://jonbarron.info/">awesome website template <3</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
